import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tempfile
import cv2
import os
import requests
import time
import sys
import subprocess
from sklearn.datasets import load_breast_cancer
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, \
    classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from PIL import Image
from ultralytics import YOLO
from sklearn.model_selection import cross_val_score

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False

# DeepSeek APIé…ç½®
DEEPSEEK_API_URL = "https://chatapi.littlewheat.com/v1/chat/completions"
DEEPSEEK_MODEL = "deepseek-chat"

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç¥ç»ç½‘ç»œåˆ†æå¹³å°",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)


# æ£€æŸ¥å¹¶å®‰è£…TensorFlowçš„å‡½æ•°
def check_and_install_tensorflow():
    try:
        import tensorflow as tf
        st.success("TensorFlow å·²å®‰è£…!")
        return True
    except ImportError:
        st.warning("TensorFlow æœªå®‰è£…ï¼Œå›¾åƒåˆ†ç±»åŠŸèƒ½å°†å—é™")

        if st.button("å°è¯•å®‰è£… TensorFlow"):
            with st.spinner("æ­£åœ¨å®‰è£… TensorFlowï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ..."):
                try:
                    # ä½¿ç”¨pipå®‰è£…tensorflow
                    subprocess.check_call([sys.executable, "-m", "pip", "install", "tensorflow"])
                    st.success("TensorFlow å®‰è£…æˆåŠŸ! è¯·é‡å¯åº”ç”¨")
                    st.experimental_rerun()
                except Exception as e:
                    st.error(f"å®‰è£…å¤±è´¥: {str(e)}")
                    st.info("""
                    **æ‰‹åŠ¨å®‰è£…æŒ‡å—:**
                    1. æ‰“å¼€å‘½ä»¤è¡Œ/ç»ˆç«¯
                    2. è¿è¡Œ: `pip install tensorflow`
                    3. é‡å¯åº”ç”¨
                    """)
        return False


# åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†
@st.cache_data
def load_breast_cancer_data():
    data = load_breast_cancer()
    df = pd.DataFrame(data.data, columns=data.feature_names)
    df['target'] = data.target
    df['diagnosis'] = df['target'].map({0: 'æ¶æ€§', 1: 'è‰¯æ€§'})
    return df, data


# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    return YOLO('models/yolov8n.pt')


# DeepSeek APIè°ƒç”¨å‡½æ•°
def analyze_with_deepseek(api_key, context, data_summary=None, prompt=None):
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œæ•°æ®åˆ†æ"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # æ„å»ºç³»ç»Ÿæç¤º
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•°æ®ç§‘å­¦å®¶ï¼Œæ“…é•¿åˆ†æåŒ»ç–—æ•°æ®é›†ç‰¹åˆ«æ˜¯ä¹³è…ºç™Œæ•°æ®ã€‚
    è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ•°æ®åˆ†ææŠ¥å‘Šå’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæä¾›æ·±å…¥çš„æ´å¯Ÿã€å»ºè®®å’Œæ€»ç»“ã€‚
    å›ç­”åº”ä¸“ä¸šã€ç®€æ´ä¸”å…·æœ‰å¯æ“ä½œæ€§ï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
    """

    # æ„å»ºç”¨æˆ·æç¤º
    user_prompt = f"""
    ### æ•°æ®åˆ†æä¸Šä¸‹æ–‡:
    {context}

    ### æ•°æ®æ‘˜è¦:
    {data_summary if data_summary else 'æ— é™„åŠ æ•°æ®æ‘˜è¦'}

    ### ç”¨æˆ·é—®é¢˜:
    {prompt if prompt else 'è¯·åˆ†ææ•°æ®å¹¶æä¾›ä¸“ä¸šè§è§£'}
    """

    payload = {
        "model": DEEPSEEK_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.3,
        "max_tokens": 1500
    }

    try:
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content']
    except Exception as e:
        return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"


# è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # äº¤å‰éªŒè¯
    cv_accuracy = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()
    cv_f1 = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted').mean()

    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)

    # ROCæ›²çº¿æ•°æ®
    fpr, tpr, roc_auc = None, None, None
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)

    return {
        'model_name': model_name,
        'model': model,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'cv_accuracy': cv_accuracy,
        'cv_f1': cv_f1,
        'train_time': train_time,
        'confusion_matrix': cm,
        'fpr': fpr,
        'tpr': tpr,
        'roc_auc': roc_auc,
        'classification_report': classification_report(y_test, y_pred, output_dict=True)
    }


# æ¨¡å‹æ¯”è¾ƒé¡µé¢
def model_comparison_page():
    st.title("ä¹³è…ºç™Œæ•°æ®é›†æœºå™¨å­¦ä¹ æ¨¡å‹æ¯”è¾ƒ")
    st.write("""
    åœ¨æ­¤é¡µé¢ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•å¯¹ä¹³è…ºç™Œæ•°æ®é›†è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬çš„æ€§èƒ½ã€‚
    """)

    # åŠ è½½æ•°æ®
    df, data = load_breast_cancer_data()

    # æ•°æ®é¢„å¤„ç†
    X = df[data.feature_names]
    y = df['target']

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    st.divider()
    st.subheader("æ•°æ®é›†ä¿¡æ¯")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"æ€»æ ·æœ¬æ•°: {len(df)}")
        st.write(f"ç‰¹å¾æ•°: {len(data.feature_names)}")
        st.write(f"è®­ç»ƒé›†å¤§å°: {X_train_scaled.shape[0]} æ ·æœ¬")
        st.write(f"æµ‹è¯•é›†å¤§å°: {X_test_scaled.shape[0]} æ ·æœ¬")

    with col2:
        diagnosis_counts = df['diagnosis'].value_counts()
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.pie(diagnosis_counts, labels=diagnosis_counts.index,
               autopct='%1.1f%%', startangle=90,
               colors=['#ff9999', '#66b3ff'])
        ax.set_title('è¯Šæ–­ç»“æœåˆ†å¸ƒ')
        st.pyplot(fig)

    st.divider()
    st.subheader("æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ")

    # é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹
    models_to_train = st.multiselect(
        "é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹:",
        options=[
            "Kè¿‘é‚»(KNN)", "å†³ç­–æ ‘", "éšæœºæ£®æ—",
            "æ”¯æŒå‘é‡æœº(SVM)", "é€»è¾‘å›å½’",
            "æ¢¯åº¦æå‡æ ‘(GBDT)", "XGBoost",
            "æ„ŸçŸ¥å™¨", "å¤šå±‚æ„ŸçŸ¥å™¨(MLP)"
        ],
        default=[
            "Kè¿‘é‚»(KNN)", "å†³ç­–æ ‘", "éšæœºæ£®æ—",
            "æ”¯æŒå‘é‡æœº(SVM)", "é€»è¾‘å›å½’"
        ]
    )

    # é«˜çº§é€‰é¡¹
    with st.expander("é«˜çº§é€‰é¡¹"):
        use_feature_selection = st.checkbox("ä½¿ç”¨ç‰¹å¾é€‰æ‹©", value=False)
        hyperparameter_tuning = st.checkbox("è¶…å‚æ•°è°ƒä¼˜", value=False)

    # ç‰¹å¾é€‰æ‹©
    if use_feature_selection:
        st.info("ä½¿ç”¨PCAè¿›è¡Œç‰¹å¾é™ç»´ï¼ˆä¿ç•™95%æ–¹å·®ï¼‰")
        pca = PCA(n_components=0.95)
        X_train_scaled = pca.fit_transform(X_train_scaled)
        X_test_scaled = pca.transform(X_test_scaled)
        st.write(f"é™ç»´åç‰¹å¾æ•°: {X_train_scaled.shape[1]}")

    # è®­ç»ƒæŒ‰é’®
    if st.button("è®­ç»ƒæ¨¡å‹", type="primary", use_container_width=True):
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        # åˆå§‹åŒ–æ¨¡å‹
        models = []
        if "Kè¿‘é‚»(KNN)" in models_to_train:
            models.append(("Kè¿‘é‚»(KNN)", KNeighborsClassifier()))
        if "å†³ç­–æ ‘" in models_to_train:
            models.append(("å†³ç­–æ ‘", DecisionTreeClassifier(random_state=42)))
        if "éšæœºæ£®æ—" in models_to_train:
            models.append(("éšæœºæ£®æ—", RandomForestClassifier(random_state=42)))
        if "æ”¯æŒå‘é‡æœº(SVM)" in models_to_train:
            models.append(("æ”¯æŒå‘é‡æœº(SVM)", SVC(probability=True, random_state=42)))
        if "é€»è¾‘å›å½’" in models_to_train:
            models.append(("é€»è¾‘å›å½’", LogisticRegression(max_iter=1000, random_state=42)))
        if "æ¢¯åº¦æå‡æ ‘(GBDT)" in models_to_train:
            models.append(("æ¢¯åº¦æå‡æ ‘(GBDT)", GradientBoostingClassifier(random_state=42)))
        if "XGBoost" in models_to_train:
            models.append(("XGBoost", XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)))
        if "æ„ŸçŸ¥å™¨" in models_to_train:
            models.append(("æ„ŸçŸ¥å™¨", Perceptron(random_state=42)))
        if "å¤šå±‚æ„ŸçŸ¥å™¨(MLP)" in models_to_train:
            models.append(
                ("å¤šå±‚æ„ŸçŸ¥å™¨(MLP)", MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=1000, random_state=42)))

        # è¶…å‚æ•°è°ƒä¼˜
        if hyperparameter_tuning:
            st.info("æ­£åœ¨è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´...")
            tuned_models = []
            for name, model in models:
                if name == "Kè¿‘é‚»(KNN)":
                    param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}
                elif name == "å†³ç­–æ ‘":
                    param_grid = {'max_depth': [3, 5, 7, 9, None]}
                elif name == "éšæœºæ£®æ—":
                    param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]}
                elif name == "æ”¯æŒå‘é‡æœº(SVM)":
                    param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale']}
                elif name == "é€»è¾‘å›å½’":
                    param_grid = {'C': [0.1, 1, 10]}
                elif name == "æ¢¯åº¦æå‡æ ‘(GBDT)":
                    param_grid = {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}
                elif name == "XGBoost":
                    param_grid = {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1], 'max_depth': [3, 5]}
                elif name == "æ„ŸçŸ¥å™¨":
                    param_grid = {'alpha': [0.0001, 0.001, 0.01], 'max_iter': [100, 500, 1000]}
                elif name == "å¤šå±‚æ„ŸçŸ¥å™¨(MLP)":
                    param_grid = {'hidden_layer_sizes': [(64,), (64, 32)], 'alpha': [0.0001, 0.001]}

                grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
                grid_search.fit(X_train_scaled, y_train)
                tuned_models.append((name, grid_search.best_estimator_))
                st.success(f"{name} æœ€ä½³å‚æ•°: {grid_search.best_params_}")

            models = tuned_models

        # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
        total_models = len(models)
        for i, (name, model) in enumerate(models):
            status_text.text(f"æ­£åœ¨è®­ç»ƒ {name} ({i + 1}/{total_models})...")
            result = train_and_evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, name)
            results.append(result)
            progress_bar.progress((i + 1) / total_models)

        # ä¿å­˜ç»“æœåˆ°session state
        st.session_state.model_results = results
        st.success("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    # æ˜¾ç¤ºç»“æœ
    if 'model_results' in st.session_state and st.session_state.model_results:
        results = st.session_state.model_results

        st.divider()
        st.subheader("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")

        # åˆ›å»ºæ€§èƒ½æ¯”è¾ƒæ•°æ®æ¡†
        metrics_df = pd.DataFrame({
            'æ¨¡å‹': [r['model_name'] for r in results],
            'å‡†ç¡®ç‡': [r['accuracy'] for r in results],
            'ç²¾ç¡®ç‡': [r['precision'] for r in results],
            'å¬å›ç‡': [r['recall'] for r in results],
            'F1åˆ†æ•°': [r['f1'] for r in results],
            'äº¤å‰éªŒè¯å‡†ç¡®ç‡': [r['cv_accuracy'] for r in results],
            'äº¤å‰éªŒè¯F1': [r['cv_f1'] for r in results],
            'è®­ç»ƒæ—¶é—´(ç§’)': [r['train_time'] for r in results]
        })

        # æ’åºå¹¶æ˜¾ç¤ºè¡¨æ ¼
        sort_by = st.selectbox("æŒ‰æŒ‡æ ‡æ’åº:",
                               ['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'å¬å›ç‡', 'ç²¾ç¡®ç‡', 'è®­ç»ƒæ—¶é—´(ç§’)'],
                               index=0)
        metrics_df = metrics_df.sort_values(by=sort_by, ascending=False)

        # ä¿®å¤ï¼šåªå¯¹æ•°å€¼åˆ—åº”ç”¨æ ¼å¼åŒ–
        numeric_cols = metrics_df.select_dtypes(include=[np.number]).columns
        styled_metrics = metrics_df.style.format("{:.4f}", subset=numeric_cols)
        st.dataframe(styled_metrics.background_gradient(cmap='Blues', subset=['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'å¬å›ç‡', 'ç²¾ç¡®ç‡']))

        # ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾è¡¨
        st.subheader("æ€§èƒ½æŒ‡æ ‡å¯è§†åŒ–")

        # é€‰æ‹©è¦å¯è§†åŒ–çš„æŒ‡æ ‡
        metric_to_plot = st.selectbox("é€‰æ‹©è¦å¯è§†åŒ–çš„æŒ‡æ ‡:",
                                      ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'äº¤å‰éªŒè¯å‡†ç¡®ç‡', 'äº¤å‰éªŒè¯F1'],
                                      index=0)

        # åˆ›å»ºæ¡å½¢å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(
            data=metrics_df,
            x='æ¨¡å‹',
            y=metric_to_plot,
            palette='viridis',
            ax=ax
        )
        ax.set_title(f'æ¨¡å‹{metric_to_plot}æ¯”è¾ƒ')
        ax.set_ylabel(metric_to_plot)
        ax.set_xlabel('æ¨¡å‹')
        plt.xticks(rotation=45)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for p in ax.patches:
            ax.annotate(f"{p.get_height():.4f}",
                        (p.get_x() + p.get_width() / 2., p.get_height()),
                        ha='center', va='center',
                        xytext=(0, 5),
                        textcoords='offset points')

        st.pyplot(fig)

        # ç»˜åˆ¶å¤šä¸ªæŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
        st.subheader("å¤šæŒ‡æ ‡æ€§èƒ½é›·è¾¾å›¾")

        # é€‰æ‹©è¦æ˜¾ç¤ºçš„æŒ‡æ ‡
        selected_metrics = st.multiselect(
            "é€‰æ‹©è¦æ¯”è¾ƒçš„æŒ‡æ ‡:",
            options=['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°'],
            default=['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
        )

        if selected_metrics:
            # è®¾ç½®é›·è¾¾å›¾è§’åº¦
            angles = np.linspace(0, 2 * np.pi, len(selected_metrics), endpoint=False).tolist()
            angles += angles[:1]  # é—­åˆå›¾å½¢

            fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

            for result in results:
                # è·å–å½“å‰æ¨¡å‹çš„æŒ‡æ ‡å€¼
                model_metrics = metrics_df[metrics_df['æ¨¡å‹'] == result['model_name']].iloc[0]
                values = [model_metrics[metric] for metric in selected_metrics]
                values += values[:1]  # é—­åˆå›¾å½¢

                ax.plot(angles, values, linewidth=1, label=result['model_name'])
                ax.fill(angles, values, alpha=0.1)

            # è®¾ç½®æ ‡ç­¾
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(selected_metrics)
            ax.set_yticklabels([])
            ax.set_title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', size=15)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))

            st.pyplot(fig)

        # ç»˜åˆ¶ROCæ›²çº¿
        st.subheader("ROCæ›²çº¿æ¯”è¾ƒ")

        fig, ax = plt.subplots(figsize=(10, 8))
        ax.plot([0, 1], [0, 1], 'k--', label='éšæœºçŒœæµ‹ (AUC=0.5)')

        for result in results:
            if result['fpr'] is not None and result['tpr'] is not None:
                ax.plot(result['fpr'], result['tpr'],
                        label=f"{result['model_name']} (AUC={result['roc_auc']:.4f})")

        ax.set_xlabel('å‡é˜³æ€§ç‡ (FPR)')
        ax.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)')
        ax.set_title('ROCæ›²çº¿')
        ax.legend(loc='lower right')

        st.pyplot(fig)

        # æ¨¡å‹è¯¦ç»†æŠ¥å‘Š
        st.divider()
        st.subheader("æ¨¡å‹è¯¦ç»†æŠ¥å‘Š")

        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š:", [r['model_name'] for r in results])
        model_result = next(r for r in results if r['model_name'] == selected_model)

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("æ··æ·†çŸ©é˜µ")
            cm = model_result['confusion_matrix']
            fig, ax = plt.subplots(figsize=(6, 5))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
            ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')
            ax.set_ylabel('çœŸå®æ ‡ç­¾')
            ax.set_title('æ··æ·†çŸ©é˜µ')
            st.pyplot(fig)

        with col2:
            st.subheader("åˆ†ç±»æŠ¥å‘Š")
            report_df = pd.DataFrame(model_result['classification_report']).transpose()

            # ä¿®å¤ï¼šåªå¯¹æ•°å€¼åˆ—åº”ç”¨æ ¼å¼åŒ–
            numeric_cols = report_df.select_dtypes(include=[np.number]).columns
            styled_report = report_df.style.format("{:.4f}", subset=numeric_cols)
            st.dataframe(styled_report.background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score']))

            st.metric("è®­ç»ƒæ—¶é—´", f"{model_result['train_time']:.4f} ç§’")
            if model_result['roc_auc'] is not None:
                st.metric("AUCåˆ†æ•°", f"{model_result['roc_auc']:.4f}")

        # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(model_result['model'], 'feature_importances_'):
            st.subheader("ç‰¹å¾é‡è¦æ€§")

            try:
                # è·å–ç‰¹å¾åç§°
                if use_feature_selection:
                    feature_names = [f"ä¸»æˆåˆ†_{i + 1}" for i in range(X_train_scaled.shape[1])]
                else:
                    feature_names = data.feature_names

                # è·å–ç‰¹å¾é‡è¦æ€§
                importances = model_result['model'].feature_importances_
                indices = np.argsort(importances)[::-1]

                # åˆ›å»ºDataFrame
                importance_df = pd.DataFrame({
                    'ç‰¹å¾': [feature_names[i] for i in indices],
                    'é‡è¦æ€§': importances[indices]
                }).head(15)

                # ç»˜åˆ¶æ¡å½¢å›¾
                fig, ax = plt.subplots(figsize=(12, 6))
                sns.barplot(
                    data=importance_df,
                    x='é‡è¦æ€§',
                    y='ç‰¹å¾',
                    palette='viridis',
                    ax=ax
                )
                ax.set_title('Top 15 é‡è¦ç‰¹å¾')
                ax.set_xlabel('é‡è¦æ€§')
                ax.set_ylabel('ç‰¹å¾')

                st.pyplot(fig)

                # æ˜¾ç¤ºè¡¨æ ¼
                st.dataframe(importance_df.style.format({'é‡è¦æ€§': '{:.4f}'}).background_gradient(cmap='Blues',
                                                                                                  subset=['é‡è¦æ€§']))
            except Exception as e:
                st.warning(f"æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§: {str(e)}")

    st.divider()
    st.info("""
    **æ€§èƒ½æŒ‡æ ‡è¯´æ˜:**
    - **å‡†ç¡®ç‡ (Accuracy)**: æ­£ç¡®é¢„æµ‹çš„æ ·æœ¬æ¯”ä¾‹
    - **ç²¾ç¡®ç‡ (Precision)**: é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­å®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
    - **å¬å›ç‡ (Recall)**: å®é™…ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­è¢«æ­£ç¡®é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹
    - **F1åˆ†æ•° (F1 Score)**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡ï¼Œç»¼åˆè¡¡é‡æ¨¡å‹æ€§èƒ½
    - **AUC (Area Under Curve)**: ROCæ›²çº¿ä¸‹é¢ç§¯ï¼Œè¡¡é‡åˆ†ç±»å™¨æ•´ä½“æ€§èƒ½
    """)


# å›¾åƒåˆ†ç±»è¿ç§»å­¦ä¹ é¡µé¢
def image_classification_page():
    st.title("å›¾åƒåˆ†ç±»è¿ç§»å­¦ä¹ ")
    st.write("""
    åœ¨æ­¤é¡µé¢ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œæ¨¡å‹å¯¹å›¾åƒæ•°æ®é›†è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚
    """)

    # æ£€æŸ¥TensorFlowæ˜¯å¦å®‰è£…
    if not check_and_install_tensorflow():
        return

    try:
        import tensorflow as tf
        from tensorflow.keras.datasets import mnist, cifar10
        from tensorflow.keras.models import Sequential, Model
        from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Input, Lambda
        from tensorflow.keras.optimizers import Adam
        from tensorflow.keras.applications import VGG16
        from tensorflow.keras.utils import to_categorical
        from tensorflow.keras.preprocessing.image import ImageDataGenerator
    except ImportError:
        st.error("TensorFlow æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å›¾åƒåˆ†ç±»åŠŸèƒ½")
        return

    # æ•°æ®é›†é€‰æ‹©
    dataset_options = {
        "MNIST (æ‰‹å†™æ•°å­—)": mnist,
        "CIFAR-10 (ç‰©ä½“è¯†åˆ«)": cifar10
    }
    selected_dataset = st.selectbox("é€‰æ‹©æ•°æ®é›†:", list(dataset_options.keys()))

    # æ¨¡å‹é€‰æ‹©
    model_options = {
        "LeNet": "lenet",
        "AlexNet": "alexnet",
        "VGG16 (è¿ç§»å­¦ä¹ )": "vgg16"
    }
    selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹æ¶æ„:", list(model_options.keys()))

    # è®­ç»ƒå‚æ•°
    st.subheader("è®­ç»ƒå‚æ•°")
    epochs = st.slider("è®­ç»ƒè½®æ•° (Epochs):", 1, 20, 5)
    batch_size = st.slider("æ‰¹å¤§å° (Batch Size):", 16, 128, 32, 16)

    # åŠ è½½æ•°æ®
    if st.button("åŠ è½½æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹", type="primary"):
        st.info(f"æ­£åœ¨åŠ è½½ {selected_dataset} æ•°æ®é›†...")

        # åŠ è½½æ•°æ®é›†
        if selected_dataset == "MNIST (æ‰‹å†™æ•°å­—)":
            (X_train, y_train), (X_test, y_test) = mnist.load_data()
            input_shape = (32, 32, 3)  # ä¿®æ”¹ä¸º32x32 RGB
            num_classes = 10

            # é¢„å¤„ç†MNISTæ•°æ®
            # 1. å°†28x28å¡«å……ä¸º32x32
            X_train = np.pad(X_train, ((0, 0), (2, 2), (2, 2)), 'constant', constant_values=0)
            X_test = np.pad(X_test, ((0, 0), (2, 2), (2, 2)), 'constant', constant_values=0)

            # 2. æ·»åŠ é€šé“ç»´åº¦
            X_train = np.expand_dims(X_train, axis=-1)
            X_test = np.expand_dims(X_test, axis=-1)

            # 3. å¤åˆ¶å•é€šé“ä¸ºä¸‰é€šé“
            X_train = np.repeat(X_train, 3, axis=-1)
            X_test = np.repeat(X_test, 3, axis=-1)

            # 4. å½’ä¸€åŒ–
            X_train = X_train.astype('float32') / 255.0
            X_test = X_test.astype('float32') / 255.0

            # 5. æ ‡ç­¾ç¼–ç 
            y_train = to_categorical(y_train, num_classes)
            y_test = to_categorical(y_test, num_classes)

        else:  # CIFAR-10
            (X_train, y_train), (X_test, y_test) = cifar10.load_data()
            input_shape = (32, 32, 3)
            num_classes = 10

            # é¢„å¤„ç†CIFAR-10æ•°æ®
            X_train = X_train.astype('float32') / 255.0
            X_test = X_test.astype('float32') / 255.0
            y_train = to_categorical(y_train, num_classes)
            y_test = to_categorical(y_test, num_classes)

        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        st.success(f"æ•°æ®é›†åŠ è½½å®Œæˆ!")
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]} æ ·æœ¬")
            st.write(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]} æ ·æœ¬")
            st.write(f"å›¾åƒå°ºå¯¸: {input_shape}")
            st.write(f"ç±»åˆ«æ•°: {num_classes}")

        with col2:
            # æ˜¾ç¤ºæ ·æœ¬å›¾åƒ
            fig, axes = plt.subplots(1, 5, figsize=(15, 3))
            for i in range(5):
                axes[i].imshow(X_train[i])
                axes[i].axis('off')
            st.pyplot(fig)

        # æ„å»ºæ¨¡å‹
        st.info(f"æ­£åœ¨æ„å»º {selected_model} æ¨¡å‹...")

        if model_options[selected_model] == "lenet":
            model = Sequential([
                Conv2D(6, (5, 5), activation='relu', input_shape=input_shape),
                MaxPooling2D(pool_size=(2, 2)),
                Conv2D(16, (5, 5), activation='relu'),
                MaxPooling2D(pool_size=(2, 2)),
                Flatten(),
                Dense(120, activation='relu'),
                Dense(84, activation='relu'),
                Dense(num_classes, activation='softmax')
            ])
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

        elif model_options[selected_model] == "alexnet":
            # ä¿®æ­£çš„AlexNetæ¨¡å‹ï¼Œé€‚ç”¨äº32x32è¾“å…¥
            model = Sequential([
                Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
                MaxPooling2D(pool_size=(2, 2)),
                Conv2D(64, (3, 3), activation='relu', padding='same'),
                MaxPooling2D(pool_size=(2, 2)),
                Conv2D(128, (3, 3), activation='relu', padding='same'),
                Conv2D(128, (3, 3), activation='relu', padding='same'),
                MaxPooling2D(pool_size=(2, 2)),
                Flatten(),
                Dense(512, activation='relu'),
                Dropout(0.5),
                Dense(num_classes, activation='softmax')
            ])
            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

        elif model_options[selected_model] == "vgg16":
            # ä¿®æ­£çš„VGG16æ¨¡å‹ï¼Œé€‚ç”¨äº32x32è¾“å…¥
            base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

            # å†»ç»“é¢„è®­ç»ƒå±‚
            for layer in base_model.layers:
                layer.trainable = False

            # æ·»åŠ æ–°çš„åˆ†ç±»å±‚
            x = base_model.output
            x = Flatten()(x)
            x = Dense(256, activation='relu')(x)
            x = Dropout(0.5)(x)
            predictions = Dense(num_classes, activation='softmax')(x)

            model = Model(inputs=base_model.input, outputs=predictions)
            model.compile(optimizer=Adam(learning_rate=0.0001),
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])

        # è®­ç»ƒæ¨¡å‹
        st.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        progress_bar = st.progress(0)
        status_text = st.empty()

        # åˆ›å»ºå®¹å™¨ç”¨äºæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        history_container = st.empty()
        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}

        # è‡ªå®šä¹‰å›è°ƒå‡½æ•°æ¥æ›´æ–°Streamlitç•Œé¢
        class StreamlitCallback(tf.keras.callbacks.Callback):
            def on_epoch_end(self, epoch, logs=None):
                history['loss'].append(logs['loss'])
                history['accuracy'].append(logs['accuracy'])
                history['val_loss'].append(logs['val_loss'])
                history['val_accuracy'].append(logs['val_accuracy'])

                # æ›´æ–°è¿›åº¦æ¡
                progress = (epoch + 1) / epochs
                progress_bar.progress(progress)
                status_text.text(
                    f"è®­ç»ƒä¸­: Epoch {epoch + 1}/{epochs} - å‡†ç¡®ç‡: {logs['accuracy']:.4f}, éªŒè¯å‡†ç¡®ç‡: {logs['val_accuracy']:.4f}")

                # æ›´æ–°è®­ç»ƒå†å²å›¾è¡¨
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

                ax1.plot(history['loss'], label='è®­ç»ƒæŸå¤±')
                ax1.plot(history['val_loss'], label='éªŒè¯æŸå¤±')
                ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
                ax1.set_xlabel('Epochs')
                ax1.set_ylabel('æŸå¤±')
                ax1.legend()

                ax2.plot(history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡')
                ax2.plot(history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡')
                ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
                ax2.set_xlabel('Epochs')
                ax2.set_ylabel('å‡†ç¡®ç‡')
                ax2.legend()

                history_container.pyplot(fig)

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train,
                  epochs=epochs,
                  batch_size=batch_size,
                  validation_split=0.2,
                  callbacks=[StreamlitCallback()],
                  verbose=0)

        # è¯„ä¼°æ¨¡å‹
        st.info("æ­£åœ¨è¯„ä¼°æ¨¡å‹...")
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        st.success(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        st.subheader("æµ‹è¯•é›†é¢„æµ‹ç¤ºä¾‹")
        n_samples = 10
        sample_indices = np.random.choice(len(X_test), n_samples)
        sample_images = X_test[sample_indices]
        sample_labels = np.argmax(y_test[sample_indices], axis=1)

        predictions = model.predict(sample_images)
        predicted_labels = np.argmax(predictions, axis=1)

        # å®šä¹‰CIFAR-10ç±»åˆ«åç§°
        cifar10_labels = [
            'é£æœº', 'æ±½è½¦', 'é¸Ÿ', 'çŒ«', 'é¹¿',
            'ç‹—', 'é’è›™', 'é©¬', 'èˆ¹', 'å¡è½¦'
        ]

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        fig, axes = plt.subplots(2, 5, figsize=(15, 6))
        axes = axes.flatten()

        for i, idx in enumerate(sample_indices):
            axes[i].imshow(X_test[idx])

            true_label = sample_labels[i]
            pred_label = predicted_labels[i]
            color = 'green' if true_label == pred_label else 'red'

            # æ ¹æ®æ•°æ®é›†é€‰æ‹©æ ‡ç­¾æ˜¾ç¤ºæ–¹å¼
            if selected_dataset == "CIFAR-10 (ç‰©ä½“è¯†åˆ«)":
                true_label_name = cifar10_labels[true_label]
                pred_label_name = cifar10_labels[pred_label]
            else:  # MNIST
                true_label_name = str(true_label)
                pred_label_name = str(pred_label)

            axes[i].set_title(f"çœŸå®: {true_label_name}\né¢„æµ‹: {pred_label_name}", color=color)
            axes[i].axis('off')
        st.pyplot(fig)

        # åˆ†æä¼ ç»Ÿç¥ç»ç½‘ç»œä¸æ·±åº¦ç½‘ç»œçš„å·®å¼‚
        st.divider()
        st.subheader("ç¥ç»ç½‘ç»œæ€§èƒ½åˆ†æ")

        st.markdown("""
        **ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼ˆæ„ŸçŸ¥å™¨ã€MLPï¼‰ä¸æ·±åº¦ç½‘ç»œï¼ˆCNNï¼‰çš„å·®å¼‚åˆ†æï¼š**

        1. **ç‰¹å¾æå–èƒ½åŠ›**:
           - ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼šéœ€è¦æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ï¼Œå¤„ç†å›¾åƒç­‰é«˜ç»´æ•°æ®æ•ˆæœæœ‰é™
           - æ·±åº¦ç½‘ç»œï¼šè‡ªåŠ¨å­¦ä¹ å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œç‰¹åˆ«é€‚åˆå›¾åƒæ•°æ®

        2. **å‚æ•°æ•°é‡**:
           - ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼šå‚æ•°è¾ƒå°‘ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
           - æ·±åº¦ç½‘ç»œï¼šå‚æ•°è¾ƒå¤šï¼Œéœ€è¦æ›´å¤šè®¡ç®—èµ„æºå’Œè®­ç»ƒæ—¶é—´

        3. **å°æ•°æ®é›†è¡¨ç°**:
           - ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼šåœ¨å°æ•°æ®é›†ä¸Šä¸å®¹æ˜“è¿‡æ‹Ÿåˆ
           - æ·±åº¦ç½‘ç»œï¼šåœ¨å°æ•°æ®é›†ä¸Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦ä½¿ç”¨æ•°æ®å¢å¼ºã€æ­£åˆ™åŒ–ç­‰æŠ€æœ¯

        4. **è¿ç§»å­¦ä¹ **:
           - ä¼ ç»Ÿç¥ç»ç½‘ç»œï¼šè¿ç§»å­¦ä¹ æ•ˆæœæœ‰é™
           - æ·±åº¦ç½‘ç»œï¼šå¯ä»¥é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œå¤§å¹…æå‡å°æ•°æ®é›†ä¸Šçš„æ€§èƒ½

        **å½“å‰å®éªŒç»“æœåˆ†æï¼š**
        - åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼Œæ·±åº¦ç½‘ç»œï¼ˆå¦‚LeNetã€AlexNetã€VGGï¼‰é€šå¸¸ä¼˜äºä¼ ç»Ÿç¥ç»ç½‘ç»œ
        - è¿ç§»å­¦ä¹ ï¼ˆå¦‚VGG16ï¼‰åœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼Œå› ä¸ºå¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçš„ç‰¹å¾æå–èƒ½åŠ›
        - å¯¹äºä¹³è…ºç™Œç­‰ç»“æ„åŒ–æ•°æ®ï¼Œä¼ ç»Ÿç¥ç»ç½‘ç»œå¯èƒ½è¡¨ç°è¶³å¤Ÿå¥½ï¼Œä¸”è®­ç»ƒæ›´å¿«
        """)

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("AIæ·±åº¦åˆ†æ")

            context = f"ç”¨æˆ·ä½¿ç”¨{selected_model}æ¨¡å‹åœ¨{selected_dataset}æ•°æ®é›†ä¸Šè¿›è¡Œäº†å›¾åƒåˆ†ç±»å®éªŒï¼Œæµ‹è¯•å‡†ç¡®ç‡ä¸º{test_acc:.4f}ã€‚"
            prompt = "è¯·åˆ†æä¼ ç»Ÿç¥ç»ç½‘ç»œä¸æ·±åº¦ç½‘ç»œåœ¨å°æ•°æ®é›†å’Œå¤æ‚ä»»åŠ¡ä¸Šçš„æ•ˆæœå·®å¼‚"

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†æ..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, prompt=prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)


# ä¾§è¾¹æ å¯¼èˆª
with st.sidebar:
    st.title("ç¥ç»ç½‘ç»œåˆ†æå¹³å°")
    st.divider()

    # æ·»åŠ DeepSeek APIå¯†é’¥è¾“å…¥
    st.subheader("å¤§æ¨¡å‹é›†æˆ")
    deepseek_key = st.text_input("DeepSeek APIå¯†é’¥", type="password",
                                 help="è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥ä»¥å¯ç”¨æ™ºèƒ½åˆ†æåŠŸèƒ½")

    # åˆ›å»ºå¯¼èˆªèœå•
    nav_selection = st.radio(
        "é€‰æ‹©åŠŸèƒ½æ¨¡å—:",
        ["é¦–é¡µ", "ä¹³è…ºç™ŒEDAåˆ†æ", "YOLOv8 ç‰©ä½“æ£€æµ‹æ¼”ç¤º", "æ¨¡å‹æ¯”è¾ƒ", "å›¾åƒåˆ†ç±»è¿ç§»å­¦ä¹ "],
        index=0
    )

    st.divider()

    # ä¹³è…ºç™ŒEDAåˆ†æç‰¹å®šè®¾ç½®
    if nav_selection == "ä¹³è…ºç™ŒEDAåˆ†æ":
        st.subheader("ä¹³è…ºç™Œåˆ†æè®¾ç½®")
        analysis_section = st.selectbox(
            "é€‰æ‹©åˆ†æéƒ¨åˆ†",
            ["æ•°æ®é›†æ¦‚è§ˆ", "ç‰¹å¾åˆ†å¸ƒ", "ç‰¹å¾ç›¸å…³æ€§", "ç‰¹å¾ä¸è¯Šæ–­å…³ç³»", "é™ç»´åˆ†æ", "åˆ†æç»“è®º"],
            index=0
        )

        features = load_breast_cancer().feature_names
        selected_features = st.multiselect(
            "é€‰æ‹©è¦åˆ†æçš„ç‰¹å¾",
            options=features,
            default=features[:5]
        )

        st.divider()
        st.info("""
        **æ•°æ®é›†ä¿¡æ¯**  
        - æ ·æœ¬æ•°: 569  
        - ç‰¹å¾æ•°: 30  
        - ç›®æ ‡å˜é‡: è¯Šæ–­ç»“æœ (0=æ¶æ€§, 1=è‰¯æ€§)  
        """)

if nav_selection == "é¦–é¡µ":
    st.title("ğŸ§  ç¥ç»ç½‘ç»œé¡¹ç›®å°å¹³å°")
    st.divider()

    st.header("æ¬¢è¿ä½¿ç”¨ä¸‰äººå°ç»„çš„ä¸€ä¸ªå…³äºç¥ç»ç½‘ç»œçš„å°å¹³å°")
    st.subheader("ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š")

    # æ¬¢è¿ä¿¡æ¯
    st.success("""
    - å·¦è¾¹é€‰æ‹©åŠŸèƒ½æ¨¡å—
    - æ‰“å¼€åŠŸèƒ½æ¨¡å—ï¼Œç»§ç»­å‘ä¸‹é€‰æ‹©ï¼Œç›´åˆ°æ»¡æ„ä¸ºæ­¢
    """)

    # æ·»åŠ ä¸€äº›è£…é¥°å…ƒç´ 
    col1, col2 = st.columns([1, 1])
    with col1:
        st.image("https://img.keaitupian.cn/newupload/11/1669620398726353.jpg")
    st.subheader("å°å¤‡æ³¨ï¼š")
    st.markdown("""
        1. **é¡¹ç›®å°ç»„æˆå‘˜**ï¼š
           - 2300131006 éƒçŠ
           - 2300131044 é™ˆæ¢¦ä½³
           - 2300131020 å°¹é”¦åº­

        2. **é¡¹ç›®åˆ†å·¥**ï¼š
           - éƒçŠï¼š
           - é™ˆæ¢¦ä½³ï¼š
           - å°¹é”¦åº­ï¼š

        """)

# ä¹³è…ºç™ŒEDAåˆ†æ
elif nav_selection == "ä¹³è…ºç™ŒEDAåˆ†æ":
    df, data = load_breast_cancer_data()

    # é¢„å…ˆè®¡ç®—è¯Šæ–­ç»“æœåˆ†å¸ƒï¼Œæ‰€æœ‰éƒ¨åˆ†éƒ½ä¼šç”¨åˆ°
    diagnosis_counts = df['diagnosis'].value_counts()

    st.title("ä¹³è…ºç™Œæ•°æ®é›†æ¢ç´¢æ€§åˆ†æ(EDA)")
    st.divider()

    # æ•°æ®é›†æ¦‚è§ˆ - ä¿æŒä¸å˜
    if analysis_section == "æ•°æ®é›†æ¦‚è§ˆ":
        st.header("æ•°æ®é›†æ¦‚è§ˆ")

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("æ•°æ®æ‘˜è¦")
            st.write(f"æ ·æœ¬æ•°: {df.shape[0]}")
            st.write(f"ç‰¹å¾æ•°: {df.shape[1] - 2} (ä¸åŒ…æ‹¬ç›®æ ‡å˜é‡)")

            st.subheader("è¯Šæ–­ç»“æœåˆ†å¸ƒ")
            fig, ax = plt.subplots(figsize=(6, 4))
            ax.pie(diagnosis_counts, labels=diagnosis_counts.index,
                   autopct='%1.1f%%', startangle=90,
                   colors=['#ff9999', '#66b3ff'])
            ax.set_title('è¯Šæ–­ç»“æœåˆ†å¸ƒ')
            st.pyplot(fig)

        with col2:
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(df.head(10))

            st.subheader("ç‰¹å¾æè¿°ç»Ÿè®¡")
            st.dataframe(df[data.feature_names].describe().T.style.background_gradient(cmap='Blues'))

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("æ™ºèƒ½æ•°æ®åˆ†æ")

            # å‡†å¤‡åˆ†ææ•°æ®
            context = "ç”¨æˆ·æ­£åœ¨æŸ¥çœ‹ä¹³è…ºç™Œæ•°æ®é›†æ¦‚è§ˆï¼ŒåŒ…æ‹¬æ•°æ®æ‘˜è¦ã€è¯Šæ–­ç»“æœåˆ†å¸ƒå’Œç‰¹å¾æè¿°ç»Ÿè®¡ã€‚"
            data_summary = f"""
            æ•°æ®é›†åŒ…å«{len(df)}ä¸ªæ ·æœ¬ï¼Œ{len(data.feature_names)}ä¸ªç‰¹å¾ã€‚
            è¯Šæ–­ç»“æœåˆ†å¸ƒ: {diagnosis_counts['æ¶æ€§']}ä¸ªæ¶æ€§æ ·æœ¬({diagnosis_counts['æ¶æ€§'] / len(df) * 100:.1f}%), 
            {diagnosis_counts['è‰¯æ€§']}ä¸ªè‰¯æ€§æ ·æœ¬({diagnosis_counts['è‰¯æ€§'] / len(df) * 100:.1f}%)ã€‚
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "è¯·åˆ†æä¹³è…ºç™Œæ•°æ®é›†çš„åŸºæœ¬ç‰¹å¾å’Œè¯Šæ–­åˆ†å¸ƒæƒ…å†µ")

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†ææ•°æ®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)

    # ç‰¹å¾åˆ†å¸ƒåˆ†æéƒ¨åˆ†
    elif analysis_section == "ç‰¹å¾åˆ†å¸ƒ":
        st.header("ç‰¹å¾åˆ†å¸ƒåˆ†æ")

        st.subheader("ç‰¹å¾ç›´æ–¹å›¾")
        cols = st.columns(3)
        for i, feature in enumerate(selected_features):
            with cols[i % 3]:
                fig, ax = plt.subplots(figsize=(6, 4))
                sns.histplot(df[feature], kde=True, ax=ax, color='skyblue')
                ax.set_title(f"{feature} åˆ†å¸ƒ")
                st.pyplot(fig)

        st.subheader("ç‰¹å¾ç®±çº¿å›¾")
        fig, ax = plt.subplots(figsize=(12, 6))
        df_melt = pd.melt(df, value_vars=selected_features)
        sns.boxplot(x='variable', y='value', data=df_melt, ax=ax)
        plt.xticks(rotation=45)
        ax.set_title('ç‰¹å¾ç®±çº¿å›¾')
        ax.set_xlabel('ç‰¹å¾')
        ax.set_ylabel('å€¼')
        st.pyplot(fig)

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("æ™ºèƒ½æ•°æ®åˆ†æ")

            # å‡†å¤‡åˆ†ææ•°æ® - æ›´æ–°ä¸ºç‰¹å¾åˆ†å¸ƒç›¸å…³å†…å®¹
            context = "ç”¨æˆ·æ­£åœ¨åˆ†æä¹³è…ºç™Œæ•°æ®é›†çš„ç‰¹å¾åˆ†å¸ƒæƒ…å†µã€‚"
            data_summary = f"""
            å·²é€‰æ‹©åˆ†æä»¥ä¸‹ç‰¹å¾: {', '.join(selected_features)}
            æ•°æ®é›†åŒ…å«{len(df)}ä¸ªæ ·æœ¬ï¼Œ{len(data.feature_names)}ä¸ªç‰¹å¾ã€‚
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "è¯·åˆ†æè¿™äº›ç‰¹å¾åˆ†å¸ƒçš„ç‰¹ç‚¹åŠå…¶å¯¹ä¹³è…ºç™Œè¯Šæ–­çš„æ„ä¹‰")

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†ææ•°æ®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)

    # ç‰¹å¾ç›¸å…³æ€§åˆ†æéƒ¨åˆ†
    elif analysis_section == "ç‰¹å¾ç›¸å…³æ€§":
        st.header("ç‰¹å¾ç›¸å…³æ€§åˆ†æ")

        st.subheader("ç‰¹å¾ç›¸å…³çŸ©é˜µ")
        corr_matrix = df[data.feature_names].corr()

        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', ax=ax)
        ax.set_title('ç‰¹å¾ç›¸å…³çŸ©é˜µ')
        st.pyplot(fig)

        st.subheader("é«˜åº¦ç›¸å…³ç‰¹å¾ (|r| > 0.8)")
        high_corr = corr_matrix.abs()
        high_corr = high_corr.unstack()
        high_corr = high_corr.sort_values(ascending=False)
        high_corr = high_corr[high_corr > 0.8]
        high_corr = high_corr[high_corr < 1]  # æ’é™¤å¯¹è§’çº¿
        high_corr = high_corr.reset_index()
        high_corr.columns = ['ç‰¹å¾1', 'ç‰¹å¾2', 'ç›¸å…³ç³»æ•°']
        st.dataframe(high_corr.drop_duplicates().style.background_gradient(cmap='YlOrRd'))

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("æ™ºèƒ½æ•°æ®åˆ†æ")

            # å‡†å¤‡åˆ†ææ•°æ® - æ›´æ–°ä¸ºç‰¹å¾ç›¸å…³æ€§ç›¸å…³å†…å®¹
            context = "ç”¨æˆ·æ­£åœ¨åˆ†æä¹³è…ºç™Œæ•°æ®é›†çš„ç‰¹å¾ç›¸å…³æ€§æƒ…å†µã€‚"
            data_summary = f"""
            å·²è¯†åˆ«å‡º{len(high_corr)}å¯¹é«˜åº¦ç›¸å…³ç‰¹å¾(|r| > 0.8)ã€‚
            æ•°æ®é›†åŒ…å«{len(df)}ä¸ªæ ·æœ¬ï¼Œ{len(data.feature_names)}ä¸ªç‰¹å¾ã€‚
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "è¯·è§£é‡Šç‰¹å¾ç›¸å…³æ€§åˆ†æç»“æœå¯¹æ„å»ºä¹³è…ºç™Œè¯Šæ–­æ¨¡å‹çš„æ„ä¹‰")

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†ææ•°æ®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)

    # ç‰¹å¾ä¸è¯Šæ–­å…³ç³»åˆ†æéƒ¨åˆ†
    elif analysis_section == "ç‰¹å¾ä¸è¯Šæ–­å…³ç³»":
        st.header("ç‰¹å¾ä¸è¯Šæ–­ç»“æœçš„å…³ç³»")

        st.subheader("è¯Šæ–­ç»“æœä¸ç‰¹å¾å…³ç³»å›¾")
        cols = st.columns(3)
        for i, feature in enumerate(selected_features):
            with cols[i % 3]:
                fig, ax = plt.subplots(figsize=(6, 4))
                sns.boxplot(x='diagnosis', y=feature, data=df, ax=ax, palette='Set2')
                ax.set_title(f"{feature} vs è¯Šæ–­ç»“æœ")
                st.pyplot(fig)

        st.subheader("ç‰¹å¾å‡å€¼å¯¹æ¯”")
        mean_features = [f for f in data.feature_names if 'mean' in f]
        benign_means = df[df['diagnosis'] == 'è‰¯æ€§'][mean_features].mean()
        malignant_means = df[df['diagnosis'] == 'æ¶æ€§'][mean_features].mean()

        fig, ax = plt.subplots(figsize=(12, 6))
        index = np.arange(len(mean_features))
        bar_width = 0.35
        ax.bar(index, benign_means, bar_width, label='è‰¯æ€§', color='#66c2a5')
        ax.bar(index + bar_width, malignant_means, bar_width, label='æ¶æ€§', color='#fc8d62')
        ax.set_xlabel('ç‰¹å¾')
        ax.set_ylabel('å‡å€¼')
        ax.set_title('è‰¯æ€§ä¸æ¶æ€§æ ·æœ¬ç‰¹å¾å‡å€¼å¯¹æ¯”')
        ax.set_xticks(index + bar_width / 2)
        ax.set_xticklabels(mean_features, rotation=90)
        ax.legend()
        st.pyplot(fig)

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("æ™ºèƒ½æ•°æ®åˆ†æ")

            # å‡†å¤‡åˆ†ææ•°æ® - æ›´æ–°ä¸ºç‰¹å¾ä¸è¯Šæ–­å…³ç³»ç›¸å…³å†…å®¹
            context = "ç”¨æˆ·æ­£åœ¨åˆ†æä¹³è…ºç™Œæ•°æ®é›†çš„ç‰¹å¾ä¸è¯Šæ–­ç»“æœçš„å…³ç³»ã€‚"
            data_summary = f"""
            å·²é€‰æ‹©åˆ†æä»¥ä¸‹ç‰¹å¾: {', '.join(selected_features)}
            å¯¹æ¯”äº†è‰¯æ€§ä¸æ¶æ€§è‚¿ç˜¤æ ·æœ¬çš„ç‰¹å¾å‡å€¼ã€‚
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "è¯·è§£é‡Šè¿™äº›ç‰¹å¾ä¸ä¹³è…ºç™Œè¯Šæ–­çš„å…³ç³»åŠå…¶ä¸´åºŠæ„ä¹‰")

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†ææ•°æ®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)

    # é™ç»´åˆ†æéƒ¨åˆ†
    elif analysis_section == "é™ç»´åˆ†æ":
        st.header("é™ç»´åˆ†æ")

        st.subheader("ä¸»æˆåˆ†åˆ†æ(PCA)")

        # æ ‡å‡†åŒ–æ•°æ®
        X = df[data.feature_names]
        y = df['diagnosis']
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # åº”ç”¨PCA
        pca = PCA(n_components=2)
        principal_components = pca.fit_transform(X_scaled)
        pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])
        pca_df['diagnosis'] = y

        # è§£é‡Šæ–¹å·®
        st.write(f"ç¬¬ä¸€ä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[0]:.2%}")
        st.write(f"ç¬¬äºŒä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[1]:.2%}")
        st.write(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")

        # PCAå¯è§†åŒ–
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.scatterplot(x='PC1', y='PC2', hue='diagnosis', data=pca_df,
                        palette={'è‰¯æ€§': 'green', 'æ¶æ€§': 'red'}, alpha=0.7, ax=ax)
        ax.set_title('PCA: å‰ä¸¤ä¸ªä¸»æˆåˆ†')
        ax.set_xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%})")
        ax.set_ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%})")
        st.pyplot(fig)

        # ç‰¹å¾é‡è¦æ€§
        st.subheader("ä¸»æˆåˆ†ç‰¹å¾é‡è¦æ€§")
        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
        loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=data.feature_names)

        fig, ax = plt.subplots(figsize=(12, 8))
        sns.heatmap(loadings_df, cmap='coolwarm', annot=True, fmt=".2f", ax=ax)
        ax.set_title('ç‰¹å¾å¯¹ä¸»æˆåˆ†çš„è´¡çŒ®')
        st.pyplot(fig)

        # æ·»åŠ å¤§æ¨¡å‹åˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("æ™ºèƒ½æ•°æ®åˆ†æ")

            # å‡†å¤‡åˆ†ææ•°æ® - æ›´æ–°ä¸ºé™ç»´åˆ†æç›¸å…³å†…å®¹
            context = "ç”¨æˆ·æ­£åœ¨å¯¹ä¹³è…ºç™Œæ•°æ®é›†è¿›è¡Œé™ç»´åˆ†æï¼ˆPCAï¼‰ã€‚"
            data_summary = f"""
            å‰ä¸¤ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}
            æ•°æ®é›†åŒ…å«{len(df)}ä¸ªæ ·æœ¬ï¼Œ{len(data.feature_names)}ä¸ªç‰¹å¾ã€‚
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "è¯·è§£é‡ŠPCAé™ç»´ç»“æœå¯¹ä¹³è…ºç™Œæ•°æ®å¯è§†åŒ–å’Œç‰¹å¾æå–çš„æ„ä¹‰")

            if st.button("è·å–AIåˆ†æ"):
                with st.spinner("AIæ­£åœ¨åˆ†ææ•°æ®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIåˆ†æç»“æœ:")
                    st.markdown(analysis_result)

    # åˆ†æç»“è®ºéƒ¨åˆ† - ä¿æŒä¸å˜
    elif analysis_section == "åˆ†æç»“è®º":
        st.header("ä¹³è…ºç™Œæ•°æ®é›†åˆ†æç»“è®º")
        st.divider()

        st.subheader("å…³é”®å‘ç°")
        st.markdown("""
        1. **è¯Šæ–­ç»“æœåˆ†å¸ƒ**ï¼š  
           - æ•°æ®é›†åŒ…å«569ä¸ªæ ·æœ¬ï¼Œå…¶ä¸­è‰¯æ€§è‚¿ç˜¤357ä¾‹(62.7%)ï¼Œæ¶æ€§è‚¿ç˜¤212ä¾‹(37.3%)
           - ç±»åˆ«åˆ†å¸ƒå­˜åœ¨è½»å¾®ä¸å¹³è¡¡ï¼Œåœ¨å»ºæ¨¡æ—¶éœ€æ³¨æ„

        2. **ç‰¹å¾åˆ†å¸ƒ**ï¼š  
           - æ‰€æœ‰ç‰¹å¾å‡ä¸ºè¿ç»­æ•°å€¼å‹å˜é‡
           - å¤§å¤šæ•°ç‰¹å¾å‘ˆç°å³ååˆ†å¸ƒï¼Œè¡¨æ˜å­˜åœ¨ä¸€äº›æç«¯å€¼
           - ä¸åŒç‰¹å¾çš„å€¼èŒƒå›´å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦æ ‡å‡†åŒ–å¤„ç†

        3. **ç‰¹å¾ç›¸å…³æ€§**ï¼š  
           - ç‰¹å¾é—´å­˜åœ¨é«˜åº¦ç›¸å…³æ€§ï¼Œç‰¹åˆ«æ˜¯åŒä¸€ç‰¹å¾çš„å‡å€¼ã€æ ‡å‡†å·®å’Œæœ€å·®å€¼ä¹‹é—´
           - ä¾‹å¦‚ï¼Œ`radius_mean`ä¸`perimeter_mean`ã€`area_mean`ç›¸å…³ç³»æ•°è¶…è¿‡0.99
           - é«˜ç›¸å…³æ€§è¡¨æ˜å¯ä»¥è¿›è¡Œç‰¹å¾é€‰æ‹©æˆ–é™ç»´ä»¥å‡å°‘å†—ä½™

        4. **ç‰¹å¾ä¸è¯Šæ–­çš„å…³ç³»**ï¼š  
           - æ¶æ€§è‚¿ç˜¤æ ·æœ¬åœ¨å¤§å¤šæ•°ç‰¹å¾ä¸Šçš„å€¼æ˜¾è‘—é«˜äºè‰¯æ€§æ ·æœ¬
           - å…³é”®åŒºåˆ†ç‰¹å¾åŒ…æ‹¬ï¼š`radius_mean`, `texture_mean`, `perimeter_mean`, `area_mean`
           - è‚¿ç˜¤å¤§å°ç›¸å…³ç‰¹å¾(åŠå¾„ã€å‘¨é•¿ã€é¢ç§¯)æ˜¯é‡è¦çš„é¢„æµ‹æŒ‡æ ‡

        5. **é™ç»´åˆ†æ**ï¼š  
           - PCAåˆ†ææ˜¾ç¤ºå‰ä¸¤ä¸ªä¸»æˆåˆ†å¯ä»¥è§£é‡Šçº¦63%çš„æ€»æ–¹å·®
           - åœ¨äºŒç»´ç©ºé—´ä¸­ï¼Œè‰¯æ€§å’Œæ¶æ€§è‚¿ç˜¤æœ‰è¾ƒå¥½çš„åˆ†ç¦»è¶‹åŠ¿
           - çº¹ç†ç‰¹å¾(`texture_mean`)å¯¹ç¬¬ä¸€ä¸»æˆåˆ†è´¡çŒ®æœ€å¤§
        """)

        st.subheader("å»ºæ¨¡å»ºè®®")
        st.markdown("""
        1. **æ•°æ®é¢„å¤„ç†**ï¼š
           - å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–
           - å¤„ç†å¯èƒ½çš„ç¦»ç¾¤å€¼
           - è€ƒè™‘å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜(è¿‡é‡‡æ ·/æ¬ é‡‡æ ·)

        2. **ç‰¹å¾å·¥ç¨‹**ï¼š
           - ä½¿ç”¨PCAæˆ–å…¶ä»–é™ç»´æŠ€æœ¯å‡å°‘ç‰¹å¾ç»´åº¦
           - ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾ä»¥å‡å°‘å¤šé‡å…±çº¿æ€§
           - åˆ›å»ºæ–°ç‰¹å¾å¦‚ç‰¹å¾æ¯”ç‡(å¦‚é¢ç§¯/åŠå¾„)

        3. **æ¨¡å‹é€‰æ‹©**ï¼š
           - é€»è¾‘å›å½’(å¯è§£é‡Šæ€§é«˜)
           - æ”¯æŒå‘é‡æœº(é€‚åˆé«˜ç»´æ•°æ®)
           - éšæœºæ£®æ—(å¤„ç†éçº¿æ€§å…³ç³»)
           - ç¥ç»ç½‘ç»œ(å¤„ç†å¤æ‚æ¨¡å¼)

        4. **æ¨¡å‹è¯„ä¼°**ï¼š
           - ä½¿ç”¨ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°(ç”±äºç±»åˆ«ä¸å¹³è¡¡)
           - ROCæ›²çº¿å’ŒAUCå€¼
           - æ··æ·†çŸ©é˜µåˆ†æ
        """)

        st.subheader("æ½œåœ¨æŒ‘æˆ˜")
        st.markdown("""
        - **ç‰¹å¾å†—ä½™**ï¼šé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯èƒ½é™ä½æŸäº›æ¨¡å‹æ€§èƒ½
        - **ç±»åˆ«ä¸å¹³è¡¡**ï¼šè‰¯æ€§æ ·æœ¬å¤šäºæ¶æ€§æ ·æœ¬ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»
        - **ç»´åº¦ç¾éš¾**ï¼š30ä¸ªç‰¹å¾ç›¸å¯¹æ ·æœ¬é‡å¯èƒ½è¾ƒå¤šï¼Œéœ€è€ƒè™‘é™ç»´
        - **è¿‡æ‹Ÿåˆé£é™©**ï¼šéœ€è¦é€‚å½“ä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯
        """)

        st.success("""
        **æ€»ç»“**ï¼šä¹³è…ºç™Œæ•°æ®é›†è´¨é‡è‰¯å¥½ï¼Œç‰¹å¾ä¸ç›®æ ‡å˜é‡æœ‰æ˜ç¡®å…³ç³»ã€‚é€šè¿‡é€‚å½“çš„ç‰¹å¾é€‰æ‹©å’Œé¢„å¤„ç†ï¼Œ
        å¯ä»¥æ„å»ºé«˜æ€§èƒ½çš„è¯Šæ–­æ¨¡å‹ã€‚å»ºè®®é‡ç‚¹å…³æ³¨è‚¿ç˜¤å¤§å°ç›¸å…³ç‰¹å¾å’Œçº¹ç†ç‰¹å¾ï¼Œå¹¶é‡‡ç”¨æ­£åˆ™åŒ–æ–¹æ³•é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
        """)

        # æ·»åŠ æ·±åº¦AIåˆ†æ
        if deepseek_key:
            st.divider()
            st.subheader("AIæ·±åº¦åˆ†ææŠ¥å‘Š")

            # å‡†å¤‡åˆ†ææ•°æ®
            context = "ç”¨æˆ·å·²å®Œæˆä¹³è…ºç™Œæ•°æ®é›†çš„å…¨é¢EDAåˆ†æï¼Œç°åœ¨éœ€è¦ä¸“ä¸šçš„æ•°æ®ç§‘å­¦å»ºè®®ã€‚"
            data_summary = """
            å…³é”®å‘ç°æ€»ç»“:
            1. è¯Šæ–­åˆ†å¸ƒ: 62.7%è‰¯æ€§, 37.3%æ¶æ€§
            2. ç‰¹å¾é«˜åº¦ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯åŒä¸€ç‰¹å¾çš„å‡å€¼ã€æ ‡å‡†å·®å’Œæœ€å·®å€¼ä¹‹é—´
            3. æ¶æ€§è‚¿ç˜¤æ ·æœ¬åœ¨å¤§å¤šæ•°ç‰¹å¾ä¸Šçš„å€¼æ˜¾è‘—é«˜äºè‰¯æ€§æ ·æœ¬
            4. PCAåˆ†ææ˜¾ç¤ºå‰ä¸¤ä¸ªä¸»æˆåˆ†å¯ä»¥è§£é‡Šçº¦63%çš„æ€»æ–¹å·®
            """

            # ç”¨æˆ·æç¤ºè¾“å…¥
            user_prompt = st.text_area("å‘AIæé—®æˆ–è¯·æ±‚åˆ†æ:",
                                       "åŸºäºåˆ†æç»“è®ºï¼Œè¯·æä¾›è¯¦ç»†çš„å»ºæ¨¡ç­–ç•¥å’ŒåŒ»ç–—åº”ç”¨å»ºè®®")

            if st.button("è·å–ä¸“ä¸šå»ºè®®"):
                with st.spinner("AIæ­£åœ¨ç”Ÿæˆä¸“ä¸šå»ºè®®..."):
                    analysis_result = analyze_with_deepseek(deepseek_key, context, data_summary, user_prompt)
                    st.success("AIä¸“ä¸šå»ºè®®:")
                    st.markdown(analysis_result)

                    # æ·»åŠ ä¸‹è½½æŠ¥å‘ŠåŠŸèƒ½
                    st.download_button(
                        label="ä¸‹è½½åˆ†ææŠ¥å‘Š",
                        data=analysis_result,
                        file_name="ä¹³è…ºç™Œåˆ†ææŠ¥å‘Š.md",
                        mime="text/markdown"
                    )

# YOLOv8 ç‰©ä½“æ£€æµ‹æ¼”ç¤º
elif nav_selection == "YOLOv8 ç‰©ä½“æ£€æµ‹æ¼”ç¤º":
    st.title("YOLOv8 ç‰©ä½“æ£€æµ‹æ¼”ç¤º")
    model = load_model()
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("ä¸Šä¼ é…ç½®")

        # é€‰æ‹©åˆ†æç±»å‹
        analysis_type = st.radio(
            "é€‰æ‹©åˆ†æç±»å‹",
            ["å›¾ç‰‡åˆ†æ", "è§†é¢‘åˆ†æ"],
            index=0,
            horizontal=True
        )

        # æ ¹æ®é€‰æ‹©ç±»å‹æ˜¾ç¤ºä¸åŒä¸Šä¼ å™¨
        if analysis_type == "å›¾ç‰‡åˆ†æ":
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ å›¾ç‰‡",
                type=['jpg', 'jpeg', 'png'],
                accept_multiple_files=False
            )
        else:  # è§†é¢‘åˆ†æ
            uploaded_file = st.file_uploader(
                "ä¸Šä¼ è§†é¢‘",
                type=['mp4', 'avi', 'mov', 'mkv'],
                accept_multiple_files=False
            )

        # æ£€æµ‹å‚æ•°
        st.subheader("æ£€æµ‹å‚æ•°")
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.25, 0.05)
        line_width = st.slider("è¾¹ç•Œæ¡†å®½åº¦", 1, 5, 2)

        # è§†é¢‘ç‰¹æœ‰å‚æ•°
        if analysis_type == "è§†é¢‘åˆ†æ":
            frame_skip = st.slider("å¸§è·³è¿‡ç‡", 1, 10, 2,
                                   help="å€¼è¶Šå¤§å¤„ç†è¶Šå¿«ä½†å¯èƒ½æ¼æ£€å¯¹è±¡")
            st.divider()
            st.info("""
                    **è§†é¢‘å¤„ç†è¯´æ˜:**
                    - å¤„ç†é€Ÿåº¦å–å†³äºè§†é¢‘é•¿åº¦å’Œå¸§è·³è¿‡ç‡
                    - å¤„ç†å®Œæˆåå¯ä¸‹è½½åˆ†æç»“æœè§†é¢‘
                    """)

        # è¯´æ˜ä¿¡æ¯
        st.divider()
        st.info("""
                ### ä½¿ç”¨è¯´æ˜:
                1. ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘æ–‡ä»¶
                2. è°ƒæ•´æ£€æµ‹å‚æ•°
                3. æŸ¥çœ‹å³ä¾§æ£€æµ‹ç»“æœ
                """)

    # ä¸»å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns(2)

    # æ·»åŠ AIåˆ†æé€‰é¡¹
    if deepseek_key:
        st.divider()
        st.subheader("AIåˆ†æè®¾ç½®")
        ai_analysis_prompt = st.text_area("AIåˆ†ææç¤º:",
                                          "è¯·æè¿°å›¾ç‰‡ä¸­çš„ç‰©ä½“åŠå…¶ç›¸äº’å…³ç³»")


    # å›¾ç‰‡åˆ†æåŠŸèƒ½
    def process_image(uploaded_file):
        # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
        image = Image.open(uploaded_file)
        img_array = np.array(image)

        with col1:
            st.subheader("åŸå§‹å›¾ç‰‡")
            st.image(image, use_column_width=True)

        # æ‰§è¡Œç‰©ä½“æ£€æµ‹
        with st.spinner("æ£€æµ‹ä¸­..."):
            results = model.predict(
                source=img_array,
                conf=conf_threshold,
                line_width=line_width,
                save=False  # ä¸åœ¨æœ¬åœ°ä¿å­˜ç»“æœ
            )

        # å¤„ç†æ£€æµ‹ç»“æœ
        if results:
            # è·å–å¸¦æ ‡æ³¨çš„å›¾åƒ
            annotated_frame = results[0].plot(line_width=line_width)
            annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

            # æ£€æµ‹ç»“æœç»Ÿè®¡
            detected_objects = []
            for result in results:
                for box in result.boxes:
                    class_id = int(box.cls)
                    class_name = model.names[class_id]
                    confidence = float(box.conf)
                    detected_objects.append({
                        "å¯¹è±¡": class_name,
                        "ç½®ä¿¡åº¦": f"{confidence:.2f}"
                    })

            with col2:
                st.subheader("æ£€æµ‹ç»“æœ")
                st.image(annotated_frame, use_column_width=True)

                if detected_objects:
                    st.subheader(f"æ£€æµ‹åˆ° {len(detected_objects)} ä¸ªå¯¹è±¡")
                    st.table(detected_objects)
                else:
                    st.warning("æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡")

        # åœ¨æ£€æµ‹ç»“æœåæ·»åŠ AIåˆ†æ
        if deepseek_key and detected_objects:
            st.divider()
            st.subheader("AIåœºæ™¯åˆ†æ")

            # æ„å»ºAIåˆ†æä¸Šä¸‹æ–‡
            context = f"ç”¨æˆ·ä¸Šä¼ äº†ä¸€å¼ å›¾ç‰‡ï¼ŒYOLOv8æ£€æµ‹åˆ°ä»¥ä¸‹ç‰©ä½“: {', '.join([obj['å¯¹è±¡'] for obj in detected_objects])}"

            if st.button("è¯·æ±‚AIåˆ†æåœºæ™¯"):
                with st.spinner("AIæ­£åœ¨åˆ†æåœºæ™¯..."):
                    analysis_result = analyze_with_deepseek(
                        deepseek_key,
                        context,
                        prompt=ai_analysis_prompt
                    )
                    st.success("åœºæ™¯åˆ†æç»“æœ:")
                    st.markdown(analysis_result)


    # è§†é¢‘åˆ†æåŠŸèƒ½
    def process_video(uploaded_file):
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ çš„è§†é¢‘
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
        temp_file.write(uploaded_file.read())
        temp_file.close()

        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(temp_file.name)

        # è·å–è§†é¢‘ä¿¡æ¯
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜å¤„ç†åçš„è§†é¢‘
        output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
        output_file.close()

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_file.name, fourcc, fps, (width, height))

        # æ˜¾ç¤ºåŸå§‹è§†é¢‘
        with col1:
            st.subheader("åŸå§‹è§†é¢‘")
            st.video(temp_file.name)

        # è¿›åº¦æ¡å’ŒçŠ¶æ€ä¿¡æ¯
        progress_bar = st.progress(0)
        status_text = st.empty()

        # å¤„ç†è§†é¢‘å¸§
        processed_frames = []
        frame_count = 0
        objects_detected = {}

        # åˆ›å»ºå®¹å™¨æ˜¾ç¤ºå®æ—¶å¤„ç†
        frame_container = col2.empty()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # è·³è¿‡æŒ‡å®šå¸§æ•°
            if frame_count % frame_skip != 0:
                continue

            # æ›´æ–°è¿›åº¦
            progress = min(100, int((frame_count / total_frames) * 100))
            progress_bar.progress(progress)
            status_text.text(f"å¤„ç†ä¸­: {frame_count}/{total_frames} å¸§ ({progress}%)")

            # æ‰§è¡Œç‰©ä½“æ£€æµ‹
            results = model.predict(
                source=frame,
                conf=conf_threshold,
                line_width=line_width,
                save=False
            )

            # å¤„ç†æ£€æµ‹ç»“æœ
            if results:
                # è·å–å¸¦æ ‡æ³¨çš„å¸§
                annotated_frame = results[0].plot(line_width=line_width)

                # æ·»åŠ åˆ°è¾“å‡ºè§†é¢‘
                out.write(annotated_frame)

                # æ›´æ–°æ£€æµ‹ç»“æœ
                for result in results:
                    for box in result.boxes:
                        class_id = int(box.cls)
                        class_name = model.names[class_id]
                        if class_name not in objects_detected:
                            objects_detected[class_name] = 0
                        objects_detected[class_name] += 1

                # æ˜¾ç¤ºå½“å‰å¤„ç†å¸§
                display_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
                frame_container.image(display_frame, caption=f"å¤„ç†å¸§: {frame_count}", use_column_width=True)

        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()

        # å®Œæˆå¤„ç†
        progress_bar.progress(100)
        status_text.text(f"å¤„ç†å®Œæˆ! å…±å¤„ç† {frame_count} å¸§")

        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        with col2:
            st.subheader("æ£€æµ‹ç»“æœ")
            st.video(output_file.name)

            # æä¾›ä¸‹è½½é“¾æ¥
            with open(output_file.name, 'rb') as f:
                video_bytes = f.read()
            st.download_button(
                label="ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                data=video_bytes,
                file_name="processed_video.mp4",
                mime="video/mp4"
            )

            # æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡
            if objects_detected:
                st.subheader("æ£€æµ‹ç»Ÿè®¡")
                st.write(f"å…±æ£€æµ‹åˆ° {sum(objects_detected.values())} ä¸ªå¯¹è±¡")

                # è½¬æ¢ä¸ºè¡¨æ ¼æ˜¾ç¤º
                detected_table = []
                for obj, count in objects_detected.items():
                    detected_table.append({
                        "å¯¹è±¡": obj,
                        "å‡ºç°æ¬¡æ•°": count
                    })
                st.table(detected_table)
            else:
                st.warning("æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file.name)
        os.unlink(output_file.name)

        # åœ¨è§†é¢‘åˆ†æåæ·»åŠ AIåˆ†æ
        if deepseek_key and objects_detected:
            st.divider()
            st.subheader("AIè§†é¢‘å†…å®¹åˆ†æ")

            # æ„å»ºAIåˆ†æä¸Šä¸‹æ–‡
            context = f"åˆ†æäº†ä¸€ä¸ªè§†é¢‘ï¼Œæ£€æµ‹åˆ°ä»¥ä¸‹ç‰©ä½“: {', '.join([f'{obj}({count}æ¬¡)' for obj, count in objects_detected.items()])}"

            if st.button("è¯·æ±‚AIåˆ†æè§†é¢‘å†…å®¹"):
                with st.spinner("AIæ­£åœ¨åˆ†æè§†é¢‘å†…å®¹..."):
                    analysis_result = analyze_with_deepseek(
                        deepseek_key,
                        context,
                        prompt="è¯·æ€»ç»“è§†é¢‘ä¸­çš„ä¸»è¦å†…å®¹å’Œç‰©ä½“äº’åŠ¨å…³ç³»"
                    )
                    st.success("è§†é¢‘å†…å®¹åˆ†æ:")
                    st.markdown(analysis_result)


    # ä¸»é€»è¾‘
    if uploaded_file is not None:
        if analysis_type == "å›¾ç‰‡åˆ†æ":
            process_image(uploaded_file)
        else:
            process_video(uploaded_file)

# æ¨¡å‹è®­ç»ƒæ¨¡å—
elif nav_selection == "æ¨¡å‹æ¯”è¾ƒ":
    model_comparison_page()

# å›¾åƒåˆ†ç±»è¿ç§»å­¦ä¹ 
elif nav_selection == "å›¾åƒåˆ†ç±»è¿ç§»å­¦ä¹ ":
    image_classification_page()


# æ·»åŠ é¡µè„š
st.divider()
st.caption("Â© 2025 ç¥ç»ç½‘ç»œåˆ†æå¹³å° | ç‰ˆæœ¬ v2.0")
